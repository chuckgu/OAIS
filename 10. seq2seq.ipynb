{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, perplexity 2.049969, output: hello Êo?\n",
      "step 1, perplexity 1.434725, output: hello þorrrrrrrr?\n",
      "step 2, perplexity 4.015453, output: hello wodddddddd?\n",
      "step 3, perplexity 6.331635, output: hello wwllllllll?\n",
      "step 4, perplexity 5.180663, output: hello wo?\n",
      "step 5, perplexity 3.632298, output: hello wodddddddd?\n",
      "step 6, perplexity 4.707955, output: hello wodddddddd?\n",
      "step 7, perplexity 1.880154, output: hello w?\n",
      "step 8, perplexity 2.443544, output: hello wo?\n",
      "step 9, perplexity 2.067893, output: hello wol?\n",
      "step 10, perplexity 0.649806, output: hello worrdrrrrr?\n",
      "step 11, perplexity 0.729922, output: hello worrdrrrrr?\n",
      "step 12, perplexity 1.195709, output: hello word?\n",
      "step 13, perplexity 0.622962, output: hello word?\n",
      "step 14, perplexity 0.631022, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 15, perplexity 0.849752, output: hello worldrrrrr?\n",
      "step 16, perplexity 0.513845, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 17, perplexity 0.395463, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 18, perplexity 0.771122, output: hello word?\n",
      "step 19, perplexity 0.495299, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 20, perplexity 0.587788, output: hello worl?\n",
      "step 21, perplexity 0.765092, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 22, perplexity 0.773903, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 23, perplexity 0.601564, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 24, perplexity 0.606479, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 25, perplexity 0.502740, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 26, perplexity 0.363772, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 27, perplexity 0.452507, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 28, perplexity 0.569117, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 29, perplexity 0.433598, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 30, perplexity 0.440993, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 31, perplexity 0.469412, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 32, perplexity 0.253495, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 33, perplexity 0.313646, output: hello word?\n",
      "step 34, perplexity 0.293265, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 35, perplexity 0.285206, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 36, perplexity 0.231933, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 37, perplexity 0.268342, output: hello wold?\n",
      "step 38, perplexity 0.248236, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 39, perplexity 0.323916, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 40, perplexity 0.388523, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 41, perplexity 0.370660, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 42, perplexity 0.239222, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 43, perplexity 0.364202, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 44, perplexity 0.278538, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 45, perplexity 0.410887, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 46, perplexity 0.427990, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 47, perplexity 0.327468, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 48, perplexity 0.288455, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 49, perplexity 0.262774, output: hello word?\n",
      "step 50, perplexity 0.224434, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 51, perplexity 0.230782, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 52, perplexity 0.218812, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 53, perplexity 0.281473, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 54, perplexity 0.235809, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 55, perplexity 0.253331, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 56, perplexity 0.347495, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 57, perplexity 0.348784, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 58, perplexity 0.257026, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 59, perplexity 0.153330, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 60, perplexity 0.411053, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 61, perplexity 0.348239, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 62, perplexity 0.474557, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 63, perplexity 0.548497, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 64, perplexity 0.547755, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 65, perplexity 0.527141, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 66, perplexity 0.416469, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 67, perplexity 0.280303, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 68, perplexity 0.257682, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 69, perplexity 0.199842, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 70, perplexity 0.277413, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 71, perplexity 0.253831, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 72, perplexity 0.285696, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 73, perplexity 0.301146, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 74, perplexity 0.266462, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 75, perplexity 0.192044, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 76, perplexity 0.230029, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 77, perplexity 0.165071, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 78, perplexity 0.214324, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 79, perplexity 0.178616, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 80, perplexity 0.151756, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 81, perplexity 0.177911, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 82, perplexity 0.167068, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 83, perplexity 0.175788, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 84, perplexity 0.161392, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 85, perplexity 0.136477, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 86, perplexity 0.166617, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 87, perplexity 0.144173, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 88, perplexity 0.150745, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 89, perplexity 0.147021, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 90, perplexity 0.135013, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 91, perplexity 0.143851, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 92, perplexity 0.129528, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 93, perplexity 0.135488, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 94, perplexity 0.128722, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 95, perplexity 0.127748, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 96, perplexity 0.120171, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 97, perplexity 0.127432, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 98, perplexity 0.124522, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n",
      "step 99, perplexity 0.118205, output: hello world?\n",
      ">>>>> success! hello world! <<<<<<<\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "vocab_size=256 # We are lazy, so we avoid fency mapping and just use one *class* per character/byte\n",
    "target_vocab_size=vocab_size\n",
    "learning_rate=0.1\n",
    "buckets=[(10, 10)] # our input and response words can be up to 10 characters long\n",
    "PAD=[0] # fill words shorter than 10 characters with 'padding' zeroes\n",
    "batch_size=10 # for parallel training (later)\n",
    "\n",
    "input_data    = [list(map(ord, \"hello\")) + PAD * 5] * batch_size\n",
    "target_data   = [list(map(ord, \"world\")) + PAD * 5] * batch_size\n",
    "target_weights= [[1.0]*6 + [0.0]*4] *batch_size # mask padding. todo: redundant --\n",
    "\n",
    "\n",
    "class BabySeq2Seq(object):\n",
    "\n",
    "    def __init__(self, source_vocab_size, target_vocab_size, buckets, size, num_layers, batch_size):\n",
    "        self.buckets = buckets\n",
    "        self.batch_size = batch_size\n",
    "        self.source_vocab_size = source_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "\n",
    "        cell = single_cell = tf.contrib.rnn.core_rnn_cell.GRUCell(size)\n",
    "        if num_layers > 1:\n",
    "                    cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
    "\n",
    "        # The seq2seq function: we use embedding for the input and attention.\n",
    "        def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "            # return (outputs, state)\n",
    "            return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                    encoder_inputs, decoder_inputs, cell,\n",
    "                    num_encoder_symbols=source_vocab_size,\n",
    "                    num_decoder_symbols=target_vocab_size,\n",
    "                    embedding_size=size,\n",
    "                    feed_previous=do_decode)\n",
    "\n",
    "        # Feeds for inputs.\n",
    "        self.encoder_inputs = []\n",
    "        self.decoder_inputs = []\n",
    "        self.target_weights = []\n",
    "        for i in range(buckets[-1][0]):\t# Last bucket is the biggest one.\n",
    "            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=\"encoder{0}\".format(i)))\n",
    "        for i in range(buckets[-1][1] + 1):\n",
    "            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=\"decoder{0}\".format(i)))\n",
    "            self.target_weights.append(tf.placeholder(tf.float32, shape=[None], name=\"weight{0}\".format(i)))\n",
    "\n",
    "        # Our targets are decoder inputs shifted by one. OK\n",
    "        targets = [self.decoder_inputs[i + 1] for i in range(len(self.decoder_inputs) - 1)]\n",
    "        self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n",
    "                self.encoder_inputs, self.decoder_inputs, targets,\n",
    "                self.target_weights, buckets,\n",
    "                lambda x, y: seq2seq_f(x, y, False))\n",
    "\n",
    "        # Gradients update operation for training the model.\n",
    "        params = tf.trainable_variables()\n",
    "        self.updates=[]\n",
    "        for b in range(len(buckets)):\n",
    "            self.updates.append(tf.train.AdamOptimizer(learning_rate).minimize(self.losses[b]))\n",
    "\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    def step(self, session, encoder_inputs, decoder_inputs, target_weights, test):\n",
    "        bucket_id=0 # for simplicity, we just use the first bucket\n",
    "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "\n",
    "        # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "        input_feed = {}\n",
    "        for l in range(encoder_size):\n",
    "            input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "        for l in range(decoder_size):\n",
    "            input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "            input_feed[self.target_weights[l].name] = target_weights[l]\n",
    "\n",
    "        # Since our targets are decoder inputs shifted by one, we need one more.\n",
    "        last_target = self.decoder_inputs[decoder_size].name\n",
    "        input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
    "\n",
    "        # Output feed: depends on whether we do a backward step or not.\n",
    "        if not test:\n",
    "            output_feed = [self.updates[bucket_id], self.losses[bucket_id]]\n",
    "        else:\n",
    "            output_feed = [self.losses[bucket_id]] # Loss for this batch.\n",
    "            for l in range(decoder_size): # Output logits.\n",
    "                output_feed.append(self.outputs[bucket_id][l])\n",
    "\n",
    "        outputs = session.run(output_feed, input_feed)\n",
    "        if not test:\n",
    "            return outputs[0], outputs[1]# Gradient norm, loss\n",
    "        else:\n",
    "            return outputs[0], outputs[1:]# loss, outputs.\n",
    "\n",
    "\n",
    "def decode(bytes):\n",
    "    return \"\".join(map(chr, bytes)).replace('\\x00', '').replace('\\n', '')\n",
    "\n",
    "step=0\n",
    "test_step=1\n",
    "with tf.Session() as session:\n",
    "    model= BabySeq2Seq(vocab_size, target_vocab_size, buckets, size=10, num_layers=1, batch_size=batch_size)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for i in range(100):\n",
    "        model.step(session, input_data, target_data, target_weights, test=False) # no outputs in training\n",
    "        if step % test_step == 0:\n",
    "            perplexity, outputs = model.step(session, input_data, target_data, target_weights, test=True)\n",
    "            words = np.argmax(outputs, axis=2)  # shape (10, 10, 256)\n",
    "            word = decode(words[0])\n",
    "            print(\"step %d, perplexity %f, output: hello %s?\" % (step, perplexity, word))\n",
    "            if word == \"world\":\n",
    "                print(\">>>>> success! hello \" + word + \"! <<<<<<<\")\n",
    "                break\n",
    "            \n",
    "        step=step+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
