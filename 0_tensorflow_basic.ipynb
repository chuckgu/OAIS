{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# TensorFlow\n",
    "\n",
    "TensorFlow는 수학 계산과 데이터의 흐름을 노드(Node)와 엣지(Edge)를 사용한 방향 그래프(Directed Graph)로 표현한다.\n",
    "노드는 수학적 계산, 데이터 입/출력, 그리고 데이터의 읽기/저장 등의 작업을 수행한다. 엣지는 노드들 간 데이터의 입출력 관계를 나타낸다.\n",
    "엣지는 동적 사이즈의 다차원 데이터 배열(=Tensor)을 실어나르는데, 여기에서 텐서플로우라는 이름이 지어졌다.\n",
    "\n",
    "# Terminology\n",
    "\n",
    "**오퍼레이션(Operation) :** \n",
    "\n",
    "그래프 상의 노드는 오퍼레이션(줄임말 op)으로 불립니다. 오퍼레이션은 하나 이상의 텐서를 받을 수 있습니다. 오퍼레이션은 계산을 수행하고, 결과를 하나 이상의 텐서로 반환할 수 있습니다.\n",
    "\n",
    "**텐서(Tensor) :** \n",
    "\n",
    "내부적으로 모든 데이터는 텐서를 통해 표현됩니다. 텐서는 일종의 다차원 배열인데, 그래프 내의 오퍼레이션 간에는 텐서만이 전달됩니다. (Caffe의 Blob과 유사합니다.)\n",
    "\n",
    "**세션(Session) :** \n",
    "\n",
    "그래프를 실행하기 위해서는 세션 객체가 필요합니다. 세션은 오퍼레이션의 실행 환경을 캡슐화한 것입니다.\n",
    "\n",
    "**변수(Variables) :**\n",
    "\n",
    "변수는 그래프의 실행시, 패러미터를 저장하고 갱신하는데 사용됩니다. 메모리 상에서 텐서를 저장하는 버퍼 역할을 합니다.\n",
    "\n",
    "# Basic Usage\n",
    "\n",
    "1. 컴퓨테이션을 그래프로 나타낸다.\n",
    "2. 그래프는 Sessions 위에서 실행한다.\n",
    "3. 데이터는 Tensor로 나타낸다.\n",
    "4. 상태를 Variables 와 함께 유지한다.\n",
    "5. 데이터를 쓰기 (연산하기) 위해 feeds와 fetches를 사용한다.\n",
    "\n",
    "# The computation graph\n",
    "\n",
    "텐서플로는 그래프를 조립하는 construction phase와 그래프의 ops를 실행하기 위해 세션을 사용하는 execution phase로 구성된다.\n",
    "\n",
    "예를 들면, 인공신경망을 학습하는 그래프는 construction phase에서 생성되고, 이것이 반복적으로 실행되면서 실질적으로 트레이닝 ops가 실행되는 것은 execution phase에서 일어난다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "graph = tf.get_default_graph()\n",
    "operations = graph.get_operations()\n",
    "print operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.ops.Operation at 0x7f6a9806cf10>,\n",
       " <tensorflow.python.framework.ops.Operation at 0x7f6a9806ced0>,\n",
       " <tensorflow.python.framework.ops.Operation at 0x7f6a9807c4d0>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix1 = tf.constant([[3., 3.]])\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "# 이렇게 쓰면, 바로 계산되는 것이 아니라, ops constructor가 default graph에 nodes를 추가한다. \n",
    "# 대부분의 경우에 이 디폴트 그래프로 전체 플로우를 나타낼 수 있다. 여러개의 그래프가 필요할 경우 Graph class 도큐먼트를 참조하자.\n",
    "\n",
    "operations = graph.get_operations()\n",
    "print operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Build the graph\n",
    "\n",
    "일단 **import** 하면 내부적으로 **default_graph_stack**에 **default Graph**가 생긴다. \n",
    "***tf.get_default_graph()***명령어로 접근 가능\n",
    "이 graph에 저장된 operation을 확인해 보면 []비어 있는 것을 알 수 있다.\n",
    "상수를 하나 선언 하면 아래처럼 주소가 기록된다.\n",
    "이러한 의미는 **operation이 리스트 형태**로 들어가 있는 것이다.\n",
    "TensorFlow는 내부적으로 ***protocol buffer***를 이용한다.\n",
    "어떤 Google 스타일의 JSON이라고 생각하면 쉽다. 위에 출력도 JSON 스럽기 때문이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"Const\"\n",
       "op: \"Const\"\n",
       "attr {\n",
       "  key: \"dtype\"\n",
       "  value {\n",
       "    type: DT_FLOAT\n",
       "  }\n",
       "}\n",
       "attr {\n",
       "  key: \"value\"\n",
       "  value {\n",
       "    tensor {\n",
       "      dtype: DT_FLOAT\n",
       "      tensor_shape {\n",
       "        dim {\n",
       "          size: 1\n",
       "        }\n",
       "        dim {\n",
       "          size: 2\n",
       "        }\n",
       "      }\n",
       "      tensor_content: \"\\000\\000@@\\000\\000@@\"\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operations[0].node_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"Const_1:0\", shape=(2, 1), dtype=float32)\n",
      "Tensor(\"MatMul:0\", shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print matrix1\n",
    "print matrix2\n",
    "print product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Launching the graph in a session\n",
    "\n",
    "이제, 디폴트 그래프는 3개의 노드를 가진다: 2개의 constant() ops 와 하나의 matmul() op. 실제로 결과를 얻기 위해서는 이 그래프를 세션에 올려야 한다. \n",
    "\n",
    "세션에서 모든 연산은 패러렐하게 작동한다. 세션 constructor에 파라메터를 넘기지 않았기 때문에 디폴트 그래프로 작동한다. 세션 API는 Session class 도큐먼트를 참고하자.\n",
    "\n",
    "작업이 끝나면 리소스를 해방하기 위해 close()가 필요하다. 파이썬에서 이러한 구조는 with 를 통해 간단하게 쓸 수 있다\n",
    "\n",
    "# Interactive Usage\n",
    "\n",
    "이와 같이, 텐서플로에는 그래프를 만들고 이를 세션에 올려서 Session.run() 을 통해 실행시킨다. 이러한 과정은 IPython 등의 interactive python environment에서는 불편할 수 있으므로 이를 위한 환경을 제공한다 - ***InteractiveSession, Tensor.eval(), Operation.run()***\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.Variable([1.0, 2.0])\n",
    "a = tf.constant([3.0, 3.0])\n",
    "\n",
    "x.initializer.run()\n",
    "\n",
    "sub = tf.sub(x, a)\n",
    "print sub.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# 결과값 'result'는 numpy의 `ndarray`로 출력된다.\n",
    "result = sess.run(product)\n",
    "print result\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  result = sess.run(product)\n",
    "  print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Variables\n",
    "\n",
    "변수는 그래프의 실행 사이에 상태를 유지한다. Variables 참고. 간단한 카운터 예제를 보자:\n",
    "\n",
    "아래 과정에서 sess.run(update)는 tf.assign(state, new_value) 라는 op인데, 이 op는 다시 new_value 노드를 호출하고 이는 tf.add(state, one) 이라는 op이다. 그리고 이는 다시 one 노드로 가서 tf.constant(1) 이라는 op를 호출할 것이다. 반면, state는 Variable이기 때문에 초기 생성자 tf.Variable(0, name=”counter”) 까지 올라가지 않고 state에 저장된 값을 사용한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 변수를 0으로 초기화\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "# state에 1을 더할 오퍼레이션 생성\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "# 그래프는 처음에 변수를 초기화해야 합니다. 아래 함수를 통해 init 오퍼레이션을 만듭니다.   \n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "# 그래프를 띄우고 오퍼레이션들을 실행\n",
    "with tf.Session() as sess:\n",
    "  # 초기화 오퍼레이션 실행\n",
    "  sess.run(init_op)\n",
    "  # state의 초기 값을 출력\n",
    "  print(sess.run(state))\n",
    "  # state를 갱신하는 오퍼레이션을 실행하고, state를 출력\n",
    "  for _ in range(3):\n",
    "    sess.run(update)\n",
    "    print(sess.run(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetches\n",
    "\n",
    "연산의 결과를 fetch 하는 (가져오는) 함수는, 이미 위에서도 많이 썼지만, run()이다. 지금까지는 싱글 노드만을 fetch했지만 여러개도 할 수 있다.\n",
    "\n",
    "# Feeds\n",
    "\n",
    "fetch가 값을 가져오는 개념이었다면, feed는 값을 넣는 개념이다. 지금까지 값을 넣기 위해 Constant와 Variable을 사용했다. 이 외에 텐서를 직접적으로 그래프의 op에 넣는 방법도 있다.\n",
    "아래와 같이 run()에서 feed를 넣어주면 넣은 tensor가 임시로 op (위 예제에서는 tf.placeholder()) 를 대체한다. 일반적으로 “feed” 를 하기 위해서 명시적으로 op를 생성하고자 할 때 tf.placeholder()를 사용한다. 위 예제 맨 위의 주석과 같이, constant와 같이 다른 op 를 대체할 수 있으나 type은 같아야 한다 (tf.constant(1) 을 하면 에러가 난다).\n",
    "\n",
    "placeholder()의 경우 feed를 넣지 않으면 에러가 난다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 14.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "input1 = tf.constant(7.)\n",
    "input2 = tf.placeholder(tf.float32, [1])\n",
    "output = tf.mul(input1, input2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  print sess.run([output], feed_dict={input2:[2.]})"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
