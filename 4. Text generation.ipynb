{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.ops.rnn_cell import BasicRNNCell,BasicLSTMCell,GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('corpus length:', 600901)\n",
      "('total chars:', 59)\n",
      "('nb sequences:', 200287)\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "path = 'data/nietzsche.txt'\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "n_char=len(chars)\n",
    "\n",
    "print('Vectorization...')\n",
    "X_data = np.zeros((len(sentences), maxlen, n_char), dtype=np.bool)\n",
    "Y_data = np.zeros((len(sentences), n_char), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X_data[i, t, char_indices[char]] = 1\n",
    "    Y_data[i, char_indices[next_chars[i]]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_iters = 1000000\n",
    "batch_size = 128\n",
    "display_step = 20\n",
    "n_hidden = 128\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, maxlen, n_char])\n",
    "y = tf.placeholder(\"float\", [None, n_char])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_char]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_char]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fc6be1bda10>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fc6be1bda10>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"model\"):\n",
    "    #tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    x_t = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x_t = tf.reshape(x_t, [-1, n_char])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x_t = tf.split(0, maxlen, x_t)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    cell = rnn_cell.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.rnn(cell, x_t, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    pred = tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "#pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.floor(size/float(batch_size)))\n",
    "    #nb_batch = int(np.ceil(size/float(batch_size)))\n",
    "    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n",
    "\n",
    "def slice_X(X, start=None, stop=None):\n",
    "    if type(X) == list:\n",
    "        if hasattr(start, '__len__'):\n",
    "            return [x[start]  for x in X]\n",
    "        else:\n",
    "            return [x[start:stop] for x in X]\n",
    "    else:\n",
    "        if hasattr(start, '__len__'):\n",
    "            return X[start]\n",
    "        else:\n",
    "            return X[start:stop]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "ins=[X_data,Y_data]\n",
    "\n",
    "n_train=X_data.shape[0]\n",
    "\n",
    "index_array = np.arange(n_train)\n",
    "\n",
    "np.random.shuffle(index_array)\n",
    "\n",
    "batches = make_batches(n_train, batch_size)\n",
    "\n",
    "ins=[slice_X(ins,index_array[batch_start:batch_end]) for batch_start, batch_end in batches]\n",
    "\n",
    "iterator=itertools.cycle((data for data in ins if data != []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2560, Minibatch Loss= 2.993518, Training Accuracy= 0.23438\n",
      "Iter 5120, Minibatch Loss= 2.761055, Training Accuracy= 0.23438\n",
      "Iter 7680, Minibatch Loss= 2.456011, Training Accuracy= 0.30469\n",
      "Iter 10240, Minibatch Loss= 2.536447, Training Accuracy= 0.25000\n",
      "Iter 12800, Minibatch Loss= 2.229813, Training Accuracy= 0.35156\n",
      "Iter 15360, Minibatch Loss= 2.640709, Training Accuracy= 0.22656\n",
      "Iter 17920, Minibatch Loss= 2.339803, Training Accuracy= 0.30469\n",
      "Iter 20480, Minibatch Loss= 2.378954, Training Accuracy= 0.32031\n",
      "Iter 23040, Minibatch Loss= 2.316076, Training Accuracy= 0.32812\n",
      "Iter 25600, Minibatch Loss= 2.357639, Training Accuracy= 0.32812\n",
      "Iter 28160, Minibatch Loss= 2.234301, Training Accuracy= 0.33594\n",
      "Iter 30720, Minibatch Loss= 2.237061, Training Accuracy= 0.38281\n",
      "Iter 33280, Minibatch Loss= 2.313208, Training Accuracy= 0.31250\n",
      "Iter 35840, Minibatch Loss= 2.147248, Training Accuracy= 0.39062\n",
      "Iter 38400, Minibatch Loss= 2.270541, Training Accuracy= 0.32031\n",
      "Iter 40960, Minibatch Loss= 2.096189, Training Accuracy= 0.37500\n",
      "Iter 43520, Minibatch Loss= 2.221933, Training Accuracy= 0.40625\n",
      "Iter 46080, Minibatch Loss= 2.168264, Training Accuracy= 0.36719\n",
      "Iter 48640, Minibatch Loss= 2.186282, Training Accuracy= 0.33594\n",
      "Iter 51200, Minibatch Loss= 2.149436, Training Accuracy= 0.36719\n",
      "Iter 53760, Minibatch Loss= 2.077058, Training Accuracy= 0.38281\n",
      "Iter 56320, Minibatch Loss= 2.174652, Training Accuracy= 0.32031\n",
      "Iter 58880, Minibatch Loss= 1.833737, Training Accuracy= 0.44531\n",
      "Iter 61440, Minibatch Loss= 2.149466, Training Accuracy= 0.36719\n",
      "Iter 64000, Minibatch Loss= 2.093473, Training Accuracy= 0.39062\n",
      "Iter 66560, Minibatch Loss= 1.974557, Training Accuracy= 0.45312\n",
      "Iter 69120, Minibatch Loss= 2.104230, Training Accuracy= 0.35156\n",
      "Iter 71680, Minibatch Loss= 2.187910, Training Accuracy= 0.38281\n",
      "Iter 74240, Minibatch Loss= 2.110708, Training Accuracy= 0.36719\n",
      "Iter 76800, Minibatch Loss= 1.989156, Training Accuracy= 0.38281\n",
      "Iter 79360, Minibatch Loss= 2.057664, Training Accuracy= 0.39062\n",
      "Iter 81920, Minibatch Loss= 2.184652, Training Accuracy= 0.33594\n",
      "Iter 84480, Minibatch Loss= 2.173903, Training Accuracy= 0.33594\n",
      "Iter 87040, Minibatch Loss= 2.120531, Training Accuracy= 0.32031\n",
      "Iter 89600, Minibatch Loss= 2.050897, Training Accuracy= 0.42188\n",
      "Iter 92160, Minibatch Loss= 1.741032, Training Accuracy= 0.48438\n",
      "Iter 94720, Minibatch Loss= 1.826072, Training Accuracy= 0.43750\n",
      "Iter 97280, Minibatch Loss= 1.974127, Training Accuracy= 0.39844\n",
      "Iter 99840, Minibatch Loss= 1.926625, Training Accuracy= 0.39062\n",
      "Iter 102400, Minibatch Loss= 1.795776, Training Accuracy= 0.50000\n",
      "Iter 104960, Minibatch Loss= 1.850983, Training Accuracy= 0.43750\n",
      "Iter 107520, Minibatch Loss= 1.712565, Training Accuracy= 0.47656\n",
      "Iter 110080, Minibatch Loss= 1.918736, Training Accuracy= 0.39844\n",
      "Iter 112640, Minibatch Loss= 1.900751, Training Accuracy= 0.46094\n",
      "Iter 115200, Minibatch Loss= 1.828804, Training Accuracy= 0.39844\n",
      "Iter 117760, Minibatch Loss= 1.951263, Training Accuracy= 0.44531\n",
      "Iter 120320, Minibatch Loss= 1.837225, Training Accuracy= 0.43750\n",
      "Iter 122880, Minibatch Loss= 1.904872, Training Accuracy= 0.46875\n",
      "Iter 125440, Minibatch Loss= 1.834704, Training Accuracy= 0.46875\n",
      "----- Generating with seed: \"on, ideas, and\n",
      "words? what matter in the\"\n",
      "on, ideas, and\n",
      "words? what matter in the sent of a self--which a prevers of a self--which a prevers of a self--which a prevers of a self--which a prevers of a self--which a prevers of a self--which a prevers of a self--which a prevers of a ()\n",
      "Iter 128000, Minibatch Loss= 1.866901, Training Accuracy= 0.40625\n",
      "Iter 130560, Minibatch Loss= 1.839016, Training Accuracy= 0.50000\n",
      "Iter 133120, Minibatch Loss= 1.703830, Training Accuracy= 0.50781\n",
      "Iter 135680, Minibatch Loss= 1.735972, Training Accuracy= 0.43750\n",
      "Iter 138240, Minibatch Loss= 1.612319, Training Accuracy= 0.53906\n",
      "Iter 140800, Minibatch Loss= 1.774115, Training Accuracy= 0.53125\n",
      "Iter 143360, Minibatch Loss= 1.652094, Training Accuracy= 0.50000\n",
      "Iter 145920, Minibatch Loss= 1.544430, Training Accuracy= 0.54688\n",
      "Iter 148480, Minibatch Loss= 1.693551, Training Accuracy= 0.49219\n",
      "Iter 151040, Minibatch Loss= 1.969277, Training Accuracy= 0.40625\n",
      "Iter 153600, Minibatch Loss= 1.904364, Training Accuracy= 0.42188\n",
      "Iter 156160, Minibatch Loss= 1.743682, Training Accuracy= 0.44531\n",
      "Iter 158720, Minibatch Loss= 1.749877, Training Accuracy= 0.50000\n",
      "Iter 161280, Minibatch Loss= 1.816895, Training Accuracy= 0.45312\n",
      "Iter 163840, Minibatch Loss= 1.788978, Training Accuracy= 0.45312\n",
      "Iter 166400, Minibatch Loss= 1.543435, Training Accuracy= 0.53125\n",
      "Iter 168960, Minibatch Loss= 2.005886, Training Accuracy= 0.40625\n",
      "Iter 171520, Minibatch Loss= 1.745208, Training Accuracy= 0.50000\n",
      "Iter 174080, Minibatch Loss= 1.964744, Training Accuracy= 0.42969\n",
      "Iter 176640, Minibatch Loss= 1.843638, Training Accuracy= 0.48438\n",
      "Iter 179200, Minibatch Loss= 1.850958, Training Accuracy= 0.46875\n",
      "Iter 181760, Minibatch Loss= 1.749091, Training Accuracy= 0.48438\n",
      "Iter 184320, Minibatch Loss= 1.601883, Training Accuracy= 0.48438\n",
      "Iter 186880, Minibatch Loss= 1.611571, Training Accuracy= 0.51562\n",
      "Iter 189440, Minibatch Loss= 1.866555, Training Accuracy= 0.46875\n",
      "Iter 192000, Minibatch Loss= 1.933888, Training Accuracy= 0.44531\n",
      "Iter 194560, Minibatch Loss= 1.637818, Training Accuracy= 0.48438\n",
      "Iter 197120, Minibatch Loss= 2.064294, Training Accuracy= 0.38281\n",
      "Iter 199680, Minibatch Loss= 1.626726, Training Accuracy= 0.55469\n",
      "Iter 202240, Minibatch Loss= 1.658414, Training Accuracy= 0.49219\n",
      "Iter 204800, Minibatch Loss= 1.597111, Training Accuracy= 0.53125\n",
      "Iter 207360, Minibatch Loss= 1.760569, Training Accuracy= 0.44531\n",
      "Iter 209920, Minibatch Loss= 1.716678, Training Accuracy= 0.50000\n",
      "Iter 212480, Minibatch Loss= 1.475464, Training Accuracy= 0.56250\n",
      "Iter 215040, Minibatch Loss= 1.876759, Training Accuracy= 0.46094\n",
      "Iter 217600, Minibatch Loss= 2.048943, Training Accuracy= 0.40625\n",
      "Iter 220160, Minibatch Loss= 1.667690, Training Accuracy= 0.51562\n",
      "Iter 222720, Minibatch Loss= 1.778175, Training Accuracy= 0.49219\n",
      "Iter 225280, Minibatch Loss= 1.690488, Training Accuracy= 0.50000\n",
      "Iter 227840, Minibatch Loss= 2.046238, Training Accuracy= 0.40625\n",
      "Iter 230400, Minibatch Loss= 1.879234, Training Accuracy= 0.45312\n",
      "Iter 232960, Minibatch Loss= 1.740536, Training Accuracy= 0.46875\n",
      "Iter 235520, Minibatch Loss= 1.803335, Training Accuracy= 0.47656\n",
      "Iter 238080, Minibatch Loss= 1.759830, Training Accuracy= 0.46875\n",
      "Iter 240640, Minibatch Loss= 1.472770, Training Accuracy= 0.57812\n",
      "Iter 243200, Minibatch Loss= 1.526391, Training Accuracy= 0.53906\n",
      "Iter 245760, Minibatch Loss= 1.580095, Training Accuracy= 0.51562\n",
      "Iter 248320, Minibatch Loss= 1.604844, Training Accuracy= 0.56250\n",
      "Iter 250880, Minibatch Loss= 1.570392, Training Accuracy= 0.51562\n",
      "Iter 253440, Minibatch Loss= 1.670764, Training Accuracy= 0.50000\n",
      "----- Generating with seed: \"s satisfaction in all circumstances, let\"\n",
      "s satisfaction in all circumstances, let the existence of the existence of the existence of the existence of the existence of the existence of the existence of the existence of the existence of the existence of the existence of the existenc()\n",
      "Iter 256000, Minibatch Loss= 1.613059, Training Accuracy= 0.51562\n",
      "Iter 258560, Minibatch Loss= 1.676013, Training Accuracy= 0.49219\n",
      "Iter 261120, Minibatch Loss= 1.591843, Training Accuracy= 0.50000\n",
      "Iter 263680, Minibatch Loss= 1.630133, Training Accuracy= 0.47656\n",
      "Iter 266240, Minibatch Loss= 1.719229, Training Accuracy= 0.48438\n",
      "Iter 268800, Minibatch Loss= 1.710077, Training Accuracy= 0.47656\n",
      "Iter 271360, Minibatch Loss= 1.607872, Training Accuracy= 0.55469\n",
      "Iter 273920, Minibatch Loss= 1.857446, Training Accuracy= 0.43750\n",
      "Iter 276480, Minibatch Loss= 1.534047, Training Accuracy= 0.57031\n",
      "Iter 279040, Minibatch Loss= 1.757113, Training Accuracy= 0.48438\n",
      "Iter 281600, Minibatch Loss= 1.551279, Training Accuracy= 0.47656\n",
      "Iter 284160, Minibatch Loss= 1.705793, Training Accuracy= 0.47656\n",
      "Iter 286720, Minibatch Loss= 1.631258, Training Accuracy= 0.52344\n",
      "Iter 289280, Minibatch Loss= 1.746720, Training Accuracy= 0.45312\n",
      "Iter 291840, Minibatch Loss= 1.601140, Training Accuracy= 0.48438\n",
      "Iter 294400, Minibatch Loss= 1.369406, Training Accuracy= 0.57031\n",
      "Iter 296960, Minibatch Loss= 1.507127, Training Accuracy= 0.53906\n",
      "Iter 299520, Minibatch Loss= 1.644045, Training Accuracy= 0.53906\n",
      "Iter 302080, Minibatch Loss= 1.726058, Training Accuracy= 0.46094\n",
      "Iter 304640, Minibatch Loss= 1.616892, Training Accuracy= 0.50000\n",
      "Iter 307200, Minibatch Loss= 1.540532, Training Accuracy= 0.57812\n",
      "Iter 309760, Minibatch Loss= 1.639628, Training Accuracy= 0.52344\n",
      "Iter 312320, Minibatch Loss= 1.472035, Training Accuracy= 0.54688\n",
      "Iter 314880, Minibatch Loss= 1.421216, Training Accuracy= 0.59375\n",
      "Iter 317440, Minibatch Loss= 1.779272, Training Accuracy= 0.48438\n",
      "Iter 320000, Minibatch Loss= 1.769511, Training Accuracy= 0.48438\n",
      "Iter 322560, Minibatch Loss= 1.708791, Training Accuracy= 0.48438\n",
      "Iter 325120, Minibatch Loss= 1.578749, Training Accuracy= 0.52344\n",
      "Iter 327680, Minibatch Loss= 1.298102, Training Accuracy= 0.60156\n",
      "Iter 330240, Minibatch Loss= 1.767710, Training Accuracy= 0.49219\n",
      "Iter 332800, Minibatch Loss= 1.482485, Training Accuracy= 0.55469\n",
      "Iter 335360, Minibatch Loss= 1.582170, Training Accuracy= 0.51562\n",
      "Iter 337920, Minibatch Loss= 1.440634, Training Accuracy= 0.57812\n",
      "Iter 340480, Minibatch Loss= 1.747224, Training Accuracy= 0.46875\n",
      "Iter 343040, Minibatch Loss= 1.478460, Training Accuracy= 0.53906\n",
      "Iter 345600, Minibatch Loss= 1.689274, Training Accuracy= 0.52344\n",
      "Iter 348160, Minibatch Loss= 1.619712, Training Accuracy= 0.52344\n",
      "Iter 350720, Minibatch Loss= 1.795155, Training Accuracy= 0.44531\n",
      "Iter 353280, Minibatch Loss= 1.615042, Training Accuracy= 0.54688\n",
      "Iter 355840, Minibatch Loss= 1.586097, Training Accuracy= 0.55469\n",
      "Iter 358400, Minibatch Loss= 1.574064, Training Accuracy= 0.48438\n",
      "Iter 360960, Minibatch Loss= 1.647326, Training Accuracy= 0.49219\n",
      "Iter 363520, Minibatch Loss= 1.603662, Training Accuracy= 0.52344\n",
      "Iter 366080, Minibatch Loss= 1.496275, Training Accuracy= 0.52344\n",
      "Iter 368640, Minibatch Loss= 1.742938, Training Accuracy= 0.50781\n",
      "Iter 371200, Minibatch Loss= 1.698754, Training Accuracy= 0.48438\n",
      "Iter 373760, Minibatch Loss= 1.276583, Training Accuracy= 0.59375\n",
      "Iter 376320, Minibatch Loss= 1.288879, Training Accuracy= 0.64062\n",
      "Iter 378880, Minibatch Loss= 1.239168, Training Accuracy= 0.64844\n",
      "Iter 381440, Minibatch Loss= 1.446804, Training Accuracy= 0.55469\n",
      "----- Generating with seed: \"at seems to the populace to be a kind\n",
      "of\"\n",
      "at seems to the populace to be a kind\n",
      "of the states the states the states the states the states the states the states the states the states the states the states the states the states the states the states the states the states the states t()\n",
      "Iter 384000, Minibatch Loss= 1.439777, Training Accuracy= 0.54688\n",
      "Iter 386560, Minibatch Loss= 1.460115, Training Accuracy= 0.57031\n",
      "Iter 389120, Minibatch Loss= 1.617143, Training Accuracy= 0.51562\n",
      "Iter 391680, Minibatch Loss= 1.630886, Training Accuracy= 0.51562\n",
      "Iter 394240, Minibatch Loss= 1.438988, Training Accuracy= 0.57031\n",
      "Iter 396800, Minibatch Loss= 1.666310, Training Accuracy= 0.48438\n",
      "Iter 399360, Minibatch Loss= 1.760270, Training Accuracy= 0.50781\n",
      "Iter 401920, Minibatch Loss= 1.510246, Training Accuracy= 0.51562\n",
      "Iter 404480, Minibatch Loss= 1.621824, Training Accuracy= 0.50781\n",
      "Iter 407040, Minibatch Loss= 1.404651, Training Accuracy= 0.58594\n",
      "Iter 409600, Minibatch Loss= 1.289955, Training Accuracy= 0.63281\n",
      "Iter 412160, Minibatch Loss= 1.423334, Training Accuracy= 0.58594\n",
      "Iter 414720, Minibatch Loss= 1.473417, Training Accuracy= 0.56250\n",
      "Iter 417280, Minibatch Loss= 1.766846, Training Accuracy= 0.43750\n",
      "Iter 419840, Minibatch Loss= 1.513542, Training Accuracy= 0.53906\n",
      "Iter 422400, Minibatch Loss= 1.451365, Training Accuracy= 0.59375\n",
      "Iter 424960, Minibatch Loss= 1.362598, Training Accuracy= 0.56250\n",
      "Iter 427520, Minibatch Loss= 1.530796, Training Accuracy= 0.52344\n",
      "Iter 430080, Minibatch Loss= 1.545785, Training Accuracy= 0.53125\n",
      "Iter 432640, Minibatch Loss= 1.569077, Training Accuracy= 0.51562\n",
      "Iter 435200, Minibatch Loss= 1.722281, Training Accuracy= 0.50000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-87fac78831f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# Run optimization op (backprop)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 382\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    383\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 655\u001b[1;33m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 723\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    724\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    728\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_step = 1000\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        [batch_x,batch_y] = iterator.next()\n",
    "        # Run optimization op (backprop)\n",
    "        _, acc, loss = sess.run([optimizer,accuracy,cost], feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "        \n",
    "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "        \n",
    "        if step % sample_step == 0:\n",
    "            generated = ''\n",
    "            sentence = text[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "            for i in range(200):\n",
    "                x_sample_input = np.zeros((1, maxlen, n_char))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_sample_input[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = sess.run(pred, feed_dict={x: x_sample_input})\n",
    "                next_index = np.argmax(preds)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()\n",
    "        \n",
    "        \n",
    "    print \"Optimization Finished!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
