{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.ops.rnn_cell import BasicRNNCell,BasicLSTMCell,GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('corpus length:', 600901)\n",
      "('total chars:', 59)\n",
      "('nb sequences:', 200287)\n",
      "Vectorization...\n",
      "pre-processing ready\n"
     ]
    }
   ],
   "source": [
    "path = 'data/nietzsche.txt'\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "n_char=len(chars)\n",
    "\n",
    "print('Vectorization...')\n",
    "X_data = np.zeros((len(sentences), maxlen, n_char), dtype=np.bool)\n",
    "Y_data = np.zeros((len(sentences), n_char), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X_data[i, t, char_indices[char]] = 1\n",
    "    Y_data[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print (\"pre-processing ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters ready\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_iters = 1000000\n",
    "batch_size = 128\n",
    "display_step = 20\n",
    "n_hidden = 128\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, maxlen, n_char])\n",
    "y = tf.placeholder(\"float\", [None, n_char])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_char]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_char]))\n",
    "}\n",
    "print (\"parameters ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fbcd4390bd0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fbcd4390bd0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network ready\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"model\"):\n",
    "    #tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    x_t = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x_t = tf.reshape(x_t, [-1, n_char])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x_t = tf.split(0, maxlen, x_t)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    cell = rnn_cell.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.rnn(cell, x_t, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    pred = tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "#pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "print (\"Network ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions ready\n"
     ]
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.floor(size/float(batch_size)))\n",
    "    #nb_batch = int(np.ceil(size/float(batch_size)))\n",
    "    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n",
    "\n",
    "def slice_X(X, start=None, stop=None):\n",
    "    if type(X) == list:\n",
    "        if hasattr(start, '__len__'):\n",
    "            return [x[start]  for x in X]\n",
    "        else:\n",
    "            return [x[start:stop] for x in X]\n",
    "    else:\n",
    "        if hasattr(start, '__len__'):\n",
    "            return X[start]\n",
    "        else:\n",
    "            return X[start:stop]  \n",
    "        \n",
    "print (\"functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets ready\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "ins=[X_data,Y_data]\n",
    "\n",
    "n_train=X_data.shape[0]\n",
    "\n",
    "index_array = np.arange(n_train)\n",
    "\n",
    "np.random.shuffle(index_array)\n",
    "\n",
    "batches = make_batches(n_train, batch_size)\n",
    "\n",
    "ins=[slice_X(ins,index_array[batch_start:batch_end]) for batch_start, batch_end in batches]\n",
    "\n",
    "iterator=itertools.cycle((data for data in ins if data != []))\n",
    "\n",
    "print (\"datasets ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2560, Minibatch Loss= 3.082875, Training Accuracy= 0.18750\n",
      "Iter 5120, Minibatch Loss= 2.748065, Training Accuracy= 0.24219\n",
      "Iter 7680, Minibatch Loss= 2.679868, Training Accuracy= 0.22656\n",
      "Iter 10240, Minibatch Loss= 2.474507, Training Accuracy= 0.29688\n",
      "Iter 12800, Minibatch Loss= 2.503165, Training Accuracy= 0.27344\n",
      "Iter 15360, Minibatch Loss= 2.576175, Training Accuracy= 0.31250\n",
      "Iter 17920, Minibatch Loss= 2.482180, Training Accuracy= 0.29688\n",
      "Iter 20480, Minibatch Loss= 2.615234, Training Accuracy= 0.27344\n",
      "Iter 23040, Minibatch Loss= 2.466130, Training Accuracy= 0.28125\n",
      "Iter 25600, Minibatch Loss= 2.455337, Training Accuracy= 0.24219\n",
      "Iter 28160, Minibatch Loss= 2.402888, Training Accuracy= 0.32031\n",
      "Iter 30720, Minibatch Loss= 2.275590, Training Accuracy= 0.32812\n",
      "Iter 33280, Minibatch Loss= 2.189733, Training Accuracy= 0.35938\n",
      "Iter 35840, Minibatch Loss= 2.345371, Training Accuracy= 0.34375\n",
      "Iter 38400, Minibatch Loss= 2.231390, Training Accuracy= 0.37500\n",
      "Iter 40960, Minibatch Loss= 2.347733, Training Accuracy= 0.26562\n",
      "Iter 43520, Minibatch Loss= 2.129396, Training Accuracy= 0.38281\n",
      "Iter 46080, Minibatch Loss= 2.287708, Training Accuracy= 0.32031\n",
      "Iter 48640, Minibatch Loss= 2.059002, Training Accuracy= 0.36719\n",
      "Iter 51200, Minibatch Loss= 2.107590, Training Accuracy= 0.39062\n",
      "Iter 53760, Minibatch Loss= 2.106169, Training Accuracy= 0.42969\n",
      "Iter 56320, Minibatch Loss= 2.298616, Training Accuracy= 0.35938\n",
      "Iter 58880, Minibatch Loss= 2.024746, Training Accuracy= 0.40625\n",
      "Iter 61440, Minibatch Loss= 2.189246, Training Accuracy= 0.37500\n",
      "Iter 64000, Minibatch Loss= 2.091506, Training Accuracy= 0.41406\n",
      "Iter 66560, Minibatch Loss= 2.131132, Training Accuracy= 0.40625\n",
      "Iter 69120, Minibatch Loss= 1.944732, Training Accuracy= 0.45312\n",
      "Iter 71680, Minibatch Loss= 2.309677, Training Accuracy= 0.32812\n",
      "Iter 74240, Minibatch Loss= 2.182583, Training Accuracy= 0.32812\n",
      "Iter 76800, Minibatch Loss= 1.923816, Training Accuracy= 0.43750\n",
      "Iter 79360, Minibatch Loss= 2.165866, Training Accuracy= 0.40625\n",
      "Iter 81920, Minibatch Loss= 1.756219, Training Accuracy= 0.43750\n",
      "Iter 84480, Minibatch Loss= 2.049535, Training Accuracy= 0.42969\n",
      "Iter 87040, Minibatch Loss= 1.881109, Training Accuracy= 0.47656\n",
      "Iter 89600, Minibatch Loss= 2.122117, Training Accuracy= 0.39062\n",
      "Iter 92160, Minibatch Loss= 1.947521, Training Accuracy= 0.41406\n",
      "Iter 94720, Minibatch Loss= 2.049644, Training Accuracy= 0.39062\n",
      "Iter 97280, Minibatch Loss= 1.883445, Training Accuracy= 0.50781\n",
      "Iter 99840, Minibatch Loss= 2.078974, Training Accuracy= 0.38281\n",
      "Iter 102400, Minibatch Loss= 1.829484, Training Accuracy= 0.43750\n",
      "Iter 104960, Minibatch Loss= 1.908378, Training Accuracy= 0.39844\n",
      "Iter 107520, Minibatch Loss= 2.010664, Training Accuracy= 0.41406\n",
      "Iter 110080, Minibatch Loss= 2.144626, Training Accuracy= 0.36719\n",
      "Iter 112640, Minibatch Loss= 1.830320, Training Accuracy= 0.49219\n",
      "Iter 115200, Minibatch Loss= 1.913620, Training Accuracy= 0.39062\n",
      "Iter 117760, Minibatch Loss= 1.917168, Training Accuracy= 0.42188\n",
      "Iter 120320, Minibatch Loss= 1.882853, Training Accuracy= 0.45312\n",
      "Iter 122880, Minibatch Loss= 1.890473, Training Accuracy= 0.36719\n",
      "Iter 125440, Minibatch Loss= 1.772443, Training Accuracy= 0.50000\n",
      "----- Generating with seed: \"heir\n",
      "loathing, with a hand which only sl\"\n",
      "heir\n",
      "loathing, with a hand which only slace the prease of the prease of the prease of the prease of the prease of the prease of the prease of the prease of the prease of the prease of the prease of the prease of the prease of the prease of ()\n",
      "Iter 128000, Minibatch Loss= 1.763174, Training Accuracy= 0.44531\n",
      "Iter 130560, Minibatch Loss= 1.869854, Training Accuracy= 0.43750\n",
      "Iter 133120, Minibatch Loss= 1.767425, Training Accuracy= 0.48438\n",
      "Iter 135680, Minibatch Loss= 1.928015, Training Accuracy= 0.41406\n",
      "Iter 138240, Minibatch Loss= 2.098334, Training Accuracy= 0.39062\n",
      "Iter 140800, Minibatch Loss= 2.001117, Training Accuracy= 0.42188\n",
      "Iter 143360, Minibatch Loss= 2.002684, Training Accuracy= 0.42969\n",
      "Iter 145920, Minibatch Loss= 1.779405, Training Accuracy= 0.51562\n",
      "Iter 148480, Minibatch Loss= 2.101756, Training Accuracy= 0.34375\n",
      "Iter 151040, Minibatch Loss= 2.051610, Training Accuracy= 0.37500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-87fac78831f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# Run optimization op (backprop)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 382\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    383\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    632\u001b[0m                 ' to a larger type (e.g. int64).')\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \"\"\"\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_step = 1000\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        [batch_x,batch_y] = iterator.next()\n",
    "        # Run optimization op (backprop)\n",
    "        _, acc, loss = sess.run([optimizer,accuracy,cost], feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "        \n",
    "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "        \n",
    "        if step % sample_step == 0:\n",
    "            generated = ''\n",
    "            sentence = text[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "            for i in range(200):\n",
    "                x_sample_input = np.zeros((1, maxlen, n_char))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_sample_input[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = sess.run(pred, feed_dict={x: x_sample_input})\n",
    "                next_index = np.argmax(preds)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()\n",
    "        \n",
    "        \n",
    "    print \"Optimization Finished!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
