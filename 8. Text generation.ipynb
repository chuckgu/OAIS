{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n",
      "total chars: 57\n",
      "nb sequences: 200285\n",
      "Vectorization...\n",
      "pre-processing ready\n"
     ]
    }
   ],
   "source": [
    "path = 'data/nietzsche.txt'\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "n_char=len(chars)\n",
    "\n",
    "print('Vectorization...')\n",
    "X_data = np.zeros((len(sentences), maxlen, n_char), dtype=np.bool)\n",
    "Y_data = np.zeros((len(sentences), n_char), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X_data[i, t, char_indices[char]] = 1\n",
    "    Y_data[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print (\"pre-processing ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters ready\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_iters = 1000000\n",
    "batch_size = 128\n",
    "display_step = 20\n",
    "n_hidden = 128\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, maxlen, n_char])\n",
    "y = tf.placeholder(\"float\", [None, n_char])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_char]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_char]))\n",
    "}\n",
    "print (\"parameters ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network ready\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"model\"):\n",
    "    #tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    #x_t = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    #x_t = tf.reshape(x_t, [-1, n_char])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    #x_t = tf.split(0, maxlen, x_t)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    else:\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, x ,time_major = True, dtype=tf.float32)\n",
    "    \n",
    "    if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "        outputs = tf.unpack(tf.transpose(outputs, [1, 0, 2]))    # states is the last outputs\n",
    "    else:\n",
    "        outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    pred = tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "#pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "print (\"Network ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions ready\n"
     ]
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.floor(size/float(batch_size)))\n",
    "    #nb_batch = int(np.ceil(size/float(batch_size)))\n",
    "    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n",
    "\n",
    "def slice_X(X, start=None, stop=None):\n",
    "    if type(X) == list:\n",
    "        if hasattr(start, '__len__'):\n",
    "            return [x[start]  for x in X]\n",
    "        else:\n",
    "            return [x[start:stop] for x in X]\n",
    "    else:\n",
    "        if hasattr(start, '__len__'):\n",
    "            return X[start]\n",
    "        else:\n",
    "            return X[start:stop]  \n",
    "        \n",
    "print (\"functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets ready\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "ins=[X_data,Y_data]\n",
    "\n",
    "n_train=X_data.shape[0]\n",
    "\n",
    "index_array = np.arange(n_train)\n",
    "\n",
    "np.random.shuffle(index_array)\n",
    "\n",
    "batches = make_batches(n_train, batch_size)\n",
    "\n",
    "ins=[slice_X(ins,index_array[batch_start:batch_end]) for batch_start, batch_end in batches]\n",
    "\n",
    "iterator=itertools.cycle((data for data in ins if data != []))\n",
    "\n",
    "print (\"datasets ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2560, Minibatch Loss= 2.927027, Training Accuracy= 0.24219\n",
      "Iter 5120, Minibatch Loss= 2.724272, Training Accuracy= 0.20312\n",
      "Iter 7680, Minibatch Loss= 2.607439, Training Accuracy= 0.19531\n",
      "Iter 10240, Minibatch Loss= 2.927746, Training Accuracy= 0.21875\n",
      "Iter 12800, Minibatch Loss= 2.551171, Training Accuracy= 0.22656\n",
      "Iter 15360, Minibatch Loss= 2.508273, Training Accuracy= 0.25000\n",
      "Iter 17920, Minibatch Loss= 2.729733, Training Accuracy= 0.23438\n",
      "Iter 20480, Minibatch Loss= 2.610261, Training Accuracy= 0.24219\n",
      "Iter 23040, Minibatch Loss= 2.675668, Training Accuracy= 0.17188\n",
      "Iter 25600, Minibatch Loss= 2.605620, Training Accuracy= 0.25781\n",
      "Iter 28160, Minibatch Loss= 2.429674, Training Accuracy= 0.32031\n",
      "Iter 30720, Minibatch Loss= 2.649277, Training Accuracy= 0.24219\n",
      "Iter 33280, Minibatch Loss= 2.338543, Training Accuracy= 0.28906\n",
      "Iter 35840, Minibatch Loss= 2.398800, Training Accuracy= 0.19531\n",
      "Iter 38400, Minibatch Loss= 2.464318, Training Accuracy= 0.26562\n",
      "Iter 40960, Minibatch Loss= 2.447124, Training Accuracy= 0.25000\n",
      "Iter 43520, Minibatch Loss= 2.626184, Training Accuracy= 0.19531\n",
      "Iter 46080, Minibatch Loss= 2.453130, Training Accuracy= 0.28906\n",
      "Iter 48640, Minibatch Loss= 2.310532, Training Accuracy= 0.28906\n",
      "Iter 51200, Minibatch Loss= 2.263381, Training Accuracy= 0.31250\n",
      "Iter 53760, Minibatch Loss= 2.465529, Training Accuracy= 0.29688\n",
      "Iter 56320, Minibatch Loss= 2.403855, Training Accuracy= 0.25000\n",
      "Iter 58880, Minibatch Loss= 2.420353, Training Accuracy= 0.27344\n",
      "Iter 61440, Minibatch Loss= 2.580256, Training Accuracy= 0.25000\n",
      "Iter 64000, Minibatch Loss= 2.573121, Training Accuracy= 0.24219\n"
     ]
    }
   ],
   "source": [
    "sample_step = 1000\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    # tf.initialize_all_variables() no long valid from\n",
    "    # 2017-03-02 if using tensorflow >= 0.12\n",
    "    if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "        init = tf.initialize_all_variables()\n",
    "    else:\n",
    "        init = tf.global_variables_initializer()\n",
    "    \n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        [batch_x,batch_y] = next(iterator)\n",
    "        # Run optimization op (backprop)\n",
    "        _, acc, loss = sess.run([optimizer,accuracy,cost], feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            print (\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "        \n",
    "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "        \n",
    "        if step % sample_step == 0:\n",
    "            generated = ''\n",
    "            sentence = text[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "            for i in range(200):\n",
    "                x_sample_input = np.zeros((1, maxlen, n_char))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_sample_input[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = sess.run(pred, feed_dict={x: x_sample_input})\n",
    "                next_index = np.argmax(preds)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()\n",
    "        \n",
    "        \n",
    "    print (\"Optimization Finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
